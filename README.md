# RAG Streaming API

A lightweight FastAPI service providing streaming Retrieval-Augmented Generation (RAG) capabilities with LangChain.

## Project Overview

This project implements a Retrieval-Augmented Generation (RAG) system with streaming API responses. It uses LangChain for the RAG pipeline, ChromaDB as the vector database, and FastAPI for the web API. The application allows users to ask questions about a knowledge base and receive answers generated by a language model, with relevant context retrieved from the vector database.

Key features:
- Streaming responses for real-time answers
- Configurable retrieval parameters
- LangChain-based RAG pipeline
- OpenAI integration for embeddings and text generation
- Docker containerization

## Project Structure

```
project_root/
├── app/
│   ├── api/
│   │   ├── __init__.py
│   │   └── endpoints.py     # API route definitions
│   ├── core/
│   │   ├── __init__.py
│   │   ├── config.py       # Application settings
│   │   └── logging.py      # Logging configuration
│   ├── models/
│   │   ├── __init__.py
│   │   └── schemas.py      # Pydantic models
│   ├── services/
│   │   ├── __init__.py
│   │   ├── create_vectordb.py  # Vector DB initialization
│   │   └── lang_rag.py    # Core RAG implementation
│   ├── __init__.py
│   ├── main.py           # FastAPI entry point
│   └── test.ipynb        # Testing notebook
├── tests/
│   ├── __init__.py
│   └── test_endpoints.py # API tests
├── chroma_langchain_db/  # Vector DB storage
├── data/                 # Data files
├── logs/                 # Log files
├── .dockerignore
├── .env                  # Environment variables
├── .gitignore
├── Dockerfile
├── README.md
├── docker-compose-local.yml
└── requirements.txt      # Python dependencies
```

## Installation

### Using Docker (Recommended)

1. Clone the repository:
   ```
   git clone <repository-url>
   cd rag_streaming_chat
   ```

2. Create a `.env` file with your OpenAI API key:
   ```
   OPENAI_API_KEY=your_openai_api_key
   ```

3. Start the application using Docker Compose:
   ```
   docker-compose -f docker-compose-local.yml up -d
   ```

### Local Installation

1. Clone the repository:
   ```
   git clone <repository-url>
   cd rag_streaming_chat
   ```

2. Create and activate a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

4. Set your OpenAI API key:
   ```
   export OPENAI_API_KEY=your_openai_api_key  # On Windows: set OPENAI_API_KEY=your_openai_api_key
   ```

5. Run the application:
   ```
   uvicorn app.main:app --reload
   ```

## Usage

### API Endpoints

#### `/query` (POST)
Processes a query and streams the response back to the client.

**Request Body:**
```json
{
  "query": "Tell me about Shekhar",
  "top_k": 3
}
```

**Parameters:**
- `query`: The question to ask the RAG system
- `top_k`: Number of context documents to retrieve (default: 3)

**Headers:**
- `x-thread-id`: (Optional) Thread ID for conversation context

**Response:**
Text stream with the generated answer.

### Example Usage

Using curl:
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"query":"What projects has Shekhar worked on?", "top_k":5}'
```

Using Python requests:
```python
import requests

response = requests.post(
    "http://localhost:8000/query",
    json={"query": "What projects has Shekhar worked on?", "top_k": 5},
    stream=True
)

for chunk in response.iter_content(chunk_size=None):
    if chunk:
        print(chunk.decode('utf-8'), end='', flush=True)
```

## Testing

Run the tests using pytest:

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_endpoints.py

# Run with verbose output
pytest -v

# Run inside Docker container
docker exec -it rag_streaming_chat-rag-api-1 pytest tests/test_endpoints.py -v
```

## Configuration

You can customize the application by modifying the settings in `app/core/config.py`. The following environment variables can be set:

- `OPENAI_API_KEY`: Your OpenAI API key (required)
- `LLM_MODEL_NAME`: OpenAI model to use (default: "gpt-4o-mini")
- `CHROMA_DB_PATH`: Path to the ChromaDB database (default: "./chroma_langchain_db")
- `LOG_LEVEL`: Logging level (default: "INFO")

## Adding Data to the Knowledge Base

The system initializes with sample documents about "Shekhar" by default. To add your own documents to the knowledge base, you would need to modify the `_init_vector_store` method in `app/services/lang_rag.py`.

## License

[Add license information here]

## Acknowledgements

This project uses the following key libraries:
- [FastAPI](https://fastapi.tiangolo.com/)
- [LangChain](https://python.langchain.com/)
- [ChromaDB](https://www.trychroma.com/)
- [OpenAI API](https://openai.com/api/)